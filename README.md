# Distributed Log Monitoring & Incident Detection System

A production-style log ingestion and anomaly detection pipeline designed to demonstrate backend infrastructure, reliability engineering, and operational debugging workflows.

This project simulates multiple backend services generating structured logs, stores them in PostgreSQL, detects error-rate anomalies, and exposes incidents through a REST API.  
The system is fully containerised using Docker and designed to resemble real production monitoring workflows.

---

## Overview

Modern distributed systems rely on strong observability pipelines to detect failures early.  
This project implements a lightweight monitoring stack that:

- Ingests logs from multiple simulated services
- Stores structured events in PostgreSQL
- Detects abnormal error spikes
- Creates incidents automatically
- Provides debugging endpoints for operational visibility

> Synthetic traffic intentionally introduces controlled failures so that incident detection can be validated in a reproducible environment.

---

## Architecture

Services run via Docker Compose:

- **generator**  
  Simulates multiple services (`auth-service`, `feed-service`, `db-service`) producing logs with latency and status codes.

- **collector (FastAPI)**  
  Receives logs via HTTP and stores them in PostgreSQL.

- **detector**  
  Polls recent logs, identifies anomaly patterns, and inserts incidents into the database.

- **db (PostgreSQL)**  
  Stores `logs` and `incidents`.

### Data Flow

generator ---> collector (/logs) ---> PostgreSQL (logs)
^
|
detector polls logs
|
PostgreSQL (incidents)
|
collector exposes /incidents & /stats


---

## Tech Stack

- Python
- FastAPI
- PostgreSQL
- Docker & Docker Compose
- Linux-based container workflows

---

## Running the Project

### Requirements

- Docker Desktop installed and running

### Start the system

```bash
docker compose up --build
Stop the system
docker compose down
API Endpoints
Health Check
GET /health
Confirms that the collector service is running.

View Incidents
GET /incidents?limit=20
Returns recent incidents generated by anomaly detection.

Example:

[
  {
    "service": "auth-service",
    "incident_type": "ERROR_SPIKE",
    "severity": "HIGH"
  }
]
Service Statistics
GET /stats?window_minutes=5
Shows error rate per service within a rolling time window.

Example output:

[
  {
    "service": "feed-service",
    "total": 2900,
    "errors": 450,
    "error_rate": 0.15
  }
]
Detection Logic
The detector continuously evaluates recent logs.

Current rule:

ERROR_SPIKE
If the number of ERROR logs for a service exceeds a threshold within the last 5 minutes:

An incident is created

Severity marked as HIGH

Time window stored for debugging

Thresholds are tuned for the synthetic workload so incidents trigger during demonstrations.

Operational Runbook
When an incident appears:

Check recent incidents:

/incidents?limit=20
Inspect error rates:

/stats?window_minutes=5
Connect to the database:

docker exec -it meta-pe-log-monitor-db-1 psql -U app -d logsdb
Investigate recent logs:

SELECT * FROM logs
WHERE service='feed-service'
ORDER BY ts DESC
LIMIT 50;
Inspect container health:

docker ps
docker logs meta-pe-log-monitor-detector-1 --tail 100
Design Goals
This project focuses on production engineering concepts rather than machine learning accuracy:

Containerised backend architecture

Reliability monitoring workflows

Incident detection and debugging

Linux/Docker-based deployment

Clear operational visibility

Future Improvements
Add alert delivery via Slack/Webhooks

Implement Prometheus metrics endpoint

Add latency anomaly detection

Introduce incident deduplication logic

Replace polling detector with streaming architecture

Author
Ahmed Farid Al Basir
MSc Artificial Intelligence â€” University of Sheffield
Focused on backend systems, production engineering, and scalable infrastructure. 
