Distributed Log Monitoring & Incident Detection System

A production-style log ingestion and anomaly detection pipeline demonstrating backend infrastructure, reliability engineering, and operational debugging workflows.

This project simulates multiple backend services generating structured logs, stores them in PostgreSQL, detects error-rate anomalies, and exposes incidents through a REST API.

The system is fully containerised using Docker and designed to resemble real production monitoring pipelines used in large-scale distributed systems.

Overview

Modern backend systems require strong observability to detect failures early.

This project implements a lightweight monitoring stack that:

Ingests logs from multiple simulated services

Stores structured events in PostgreSQL

Detects abnormal error spikes

Automatically creates incidents

Exposes debugging and monitoring endpoints

Synthetic traffic intentionally introduces controlled failures so anomaly detection can be validated in a reproducible environment.

Architecture

Services run using Docker Compose:

generator
Simulates backend services (auth-service, feed-service, db-service) producing structured logs.

collector (FastAPI)
Receives logs via REST API and stores them in PostgreSQL.

detector
Continuously analyses recent logs and creates incidents when anomalies occur.

db (PostgreSQL)
Stores log events and detected incidents.

Data Flow

generator → collector (/logs) → PostgreSQL (logs)
detector → scans logs → PostgreSQL (incidents)
collector → exposes /incidents and /stats

Tech Stack

Python

FastAPI

PostgreSQL

Docker & Docker Compose

Linux-based container workflows

Running the Project
Requirements

Docker Desktop installed and running

Start the system
docker compose up --build

Stop the system
docker compose down

API Endpoints
Health Check
GET /health


Confirms the collector service is running.

View Incidents
GET /incidents?limit=20


Returns recent incidents generated by anomaly detection.

Example:

[
  {
    "service": "auth-service",
    "incident_type": "ERROR_SPIKE",
    "severity": "HIGH"
  }
]

Service Statistics
GET /stats?window_minutes=5


Shows error rate per service within a rolling time window.

Example:

[
  {
    "service": "feed-service",
    "total": 2900,
    "errors": 450,
    "error_rate": 0.15
  }
]

Detection Logic

The detector continuously evaluates recent logs.

Current rule:

ERROR_SPIKE

If the number of ERROR logs for a service exceeds a threshold within the last 5 minutes:

An incident is created

Severity marked as HIGH

Time window stored for debugging

Thresholds are tuned for synthetic workload so incidents trigger during demonstrations.

Operational Runbook

When an incident appears:

Check incidents:

http://localhost:8000/incidents?limit=20


Inspect error rates:

http://localhost:8000/stats?window_minutes=5


Connect to the database:

docker exec -it meta-pe-log-monitor-db-1 psql -U app -d logsdb


Inspect recent logs:

SELECT * FROM logs
WHERE service='feed-service'
ORDER BY ts DESC
LIMIT 50;


Check container health:

docker ps
docker logs meta-pe-log-monitor-detector-1 --tail 100

Design Goals

This project focuses on production engineering concepts rather than machine learning:

Containerised backend architecture

Reliability monitoring workflows

Incident detection and debugging

Linux/Docker deployment

Clear operational visibility

Future Improvements

Add Slack/Webhook alert delivery

Add Prometheus metrics endpoint

Introduce latency anomaly detection

Implement incident deduplication

Replace polling detector with streaming architecture

Author

Ahmed Farid Al Basir
MSc Artificial Intelligence — University of Sheffield

Focused on backend systems, production engineering, and scalable infrastructure.
